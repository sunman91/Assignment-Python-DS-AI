{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa1cec5",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Your work: Let's modify the above scratch code:\n",
    "- Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
    "- Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\n",
    "- Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.   Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index\n",
    "- Put everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57b2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0837c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07a994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84afbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Determines whether threshold should be evaluated as < or >\n",
    "        self.polarity = 1\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        # Voting power of the stump\n",
    "        self.alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8ecb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump():\n",
    "    def __init__(self, num_stumps=5, eta=0.5):\n",
    "        self.num_stumps = num_stumps\n",
    "        self.eta = eta\n",
    "        self.models = [DecisionStump() for _ in range(self.num_stumps)]\n",
    "        \n",
    "    def fit(self, X, y): #<----X_train, y_train\n",
    "        m,n = X.shape\n",
    "        \n",
    "        #initially, we set our weight to 1/m\n",
    "        W = np.full(m, 1/m)\n",
    "        \n",
    "        #Loop over all models we created\n",
    "        for clf in self.models:    \n",
    "    \n",
    "          #highest possible value so the first is always less than this\n",
    "            min_err = np.inf\n",
    "            \n",
    "            #Loop for all features and all thresholds in X and find its error             \n",
    "            for feature in range(n):\n",
    "                #sort the values so we can divide by 2 to get midpoinst\n",
    "                feature_vals = np.sort(X[:, feature])\n",
    "                \n",
    "                #get all the thresholds\n",
    "                threshold_list = (feature_vals[:-1] + feature_vals[1:])/2\n",
    "                \n",
    "                \n",
    "                for threshold in threshold_list:\n",
    "                    #Try both polarity becuase we set all to 1 at first\n",
    "                    #1 or -1 could on either side of the threshold\n",
    "                    for polarity in [1, -1]:\n",
    "                        yhat = np.ones(len(y)) #set all to 1\n",
    "                        \n",
    "                        #polarity=1 rule \n",
    "                        #separate the two classes at given threshold\n",
    "                        yhat[polarity * X[:, feature] < polarity * threshold] = -1  \n",
    "                        err = W[(yhat != y)].sum()\n",
    "                                        \n",
    "                        #to save the best stump\n",
    "                        if err < min_err:\n",
    "                            clf.polarity = polarity\n",
    "                            clf.threshold = threshold\n",
    "                            clf.feature_index = feature\n",
    "                            min_err = err\n",
    "        \n",
    "            #only the best stump(threshold) for the feature will be saved in the Decision Stump class\n",
    "            #we calculate its alpha, and reweight samples\n",
    "            \n",
    "            lap = 1e-5 #to prevent division by zero\n",
    "            term2 = np.log ((1 - min_err) / (min_err + lap))\n",
    "            clf.alpha = self.eta * term2  \n",
    "            \n",
    "            W = W * np.exp(-clf.alpha * y * yhat)/sum (W)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        m, n = X.shape\n",
    "        yhat = np.zeros(m)\n",
    "        #for clf in self.clf_list:\n",
    "        for clf in self.models:\n",
    "            pred = np.ones(m) #set all to 1\n",
    "            pred[clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold] = -1  #polarity=1 rule\n",
    "            yhat = yhat + clf.alpha * pred\n",
    "\n",
    "        return np.sign(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32ee74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.99      0.83        79\n",
      "           1       0.98      0.56      0.71        71\n",
      "\n",
      "    accuracy                           0.79       150\n",
      "   macro avg       0.85      0.78      0.77       150\n",
      "weighted avg       0.84      0.79      0.78       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Stump(num_stumps=10, eta = 0.6)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba18f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
